{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: 04_transformar_dados.ipynb\n",
    "\n",
    "Este notebook tem como objetivo:\n",
    "1. Ler o arquivo `nafld1.csv` do bucket `raw` no MinIO.\n",
    "2. Transformar o arquivo para o formato Parquet e salvá-lo no bucket `bronze`.\n",
    "3. Realizar uma análise inicial exploratória (EDA) para entender melhor os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/13 14:24:22 WARN Utils: Your hostname, DESKTOP-78HVMSD resolves to a loopback address: 127.0.1.1; using 172.26.49.215 instead (on interface eth0)\n",
      "25/03/13 14:24:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 14:24:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/13 14:24:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 's3a://raw/nafld1.csv' carregado com sucesso!\n",
      "Colunas detectadas automaticamente: ['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9']\n",
      "+---+---+----+------+------+----------------+-------+------+------+\n",
      "|id |age|male|weight|height|bmi             |case_id|futime|status|\n",
      "+---+---+----+------+------+----------------+-------+------+------+\n",
      "|id |age|male|weight|height|bmi             |case.id|futime|status|\n",
      "|1  |57 |0   |60    |163   |22.6909394821587|10630  |6261  |0     |\n",
      "|2  |67 |0   |70.4  |168   |24.8840277061027|14817  |624   |0     |\n",
      "|3  |53 |1   |105.8 |186   |30.4535370503297|3      |1783  |0     |\n",
      "|4  |56 |1   |109.3 |170   |37.8300998173156|6628   |3143  |0     |\n",
      "+---+---+----+------+------+----------------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- male: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- bmi: string (nullable = true)\n",
      " |-- case_id: string (nullable = true)\n",
      " |-- futime: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "Número de linhas: 17550, Número de colunas: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 14:24:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-------------------+\n",
      "|summary|               id|               age|               male|            weight|            height|               bmi|          case_id|           futime|             status|\n",
      "+-------+-----------------+------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-------------------+\n",
      "|  count|            17550|             17550|              17550|             12764|             14382|             12589|            17519|            17550|              17550|\n",
      "|   mean|8784.215966721751| 52.65963872585333|0.46732007521796115| 86.35334952597289|169.43494889089772|30.073864900657256|  8840.9244776801|2410.600547039717|0.07772522650863298|\n",
      "| stddev|5070.970712376356|14.722515127574011| 0.4989450954701136|22.239458031175896|10.141421399552367| 7.085435227244808|5051.334075260882|1573.017637978383|0.26774633647846907|\n",
      "|    min|                1|                18|                  0|               100|               123|    10.92133973141|              100|               10|                  0|\n",
      "|    max|               id|               age|               male|            weight|            height|               bmi|          case.id|           futime|             status|\n",
      "+-------+-----------------+------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuração do Spark para trabalhar com MinIO\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transformação de Dados\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ler o arquivo CSV do bucket 'raw', ignorando o cabeçalho original\n",
    "file_path = \"s3a://raw/nafld1.csv\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv(\n",
    "        file_path,\n",
    "        header=False,         # Ignorar o cabeçalho\n",
    "        inferSchema=True,     # Inferir os tipos de dados\n",
    "        nullValue=\"NA\"        # Tratar \"NA\" como valores nulos\n",
    "    )\n",
    "    print(f\"Arquivo '{file_path}' carregado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o arquivo CSV: {e}\")\n",
    "\n",
    "# Exibir as colunas detectadas inicialmente\n",
    "print(\"Colunas detectadas automaticamente:\", df.columns)\n",
    "\n",
    "# Renomear as colunas (ajustar manualmente caso tenha uma a mais)\n",
    "colunas = [\"_c0\", \"id\", \"age\", \"male\", \"weight\", \"height\", \"bmi\", \"case_id\", \"futime\", \"status\"]\n",
    "\n",
    "if len(df.columns) == len(colunas):  # Verifica se o número de colunas corresponde\n",
    "    df = df.toDF(*colunas)  # Renomear com os nomes definidos\n",
    "    # Remover a coluna desnecessária se existir\n",
    "    if \"_c0\" in df.columns:\n",
    "        df = df.drop(\"_c0\")  # Remova _c0 caso seja apenas um índice ou algo irrelevante\n",
    "else:\n",
    "    print(\"Aviso: Número de colunas detectado não corresponde ao esperado.\")\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Exibir o esquema do DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Contar o número de linhas e colunas\n",
    "print(f\"Número de linhas: {df.count()}, Número de colunas: {len(df.columns)}\")\n",
    "\n",
    "# Exibir estatísticas descritivas\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo com sucesso no bucket 'bronze' em s3a://bronze/nafld1.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Caminho para salvar o arquivo Parquet no bucket 'bronze'\n",
    "output_path = \"s3a://bronze/nafld1.parquet\"\n",
    "\n",
    "try:\n",
    "    # Salvar o DataFrame em formato Parquet\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"Arquivo salvo com sucesso no bucket 'bronze' em {output_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar o arquivo Parquet no bucket 'bronze': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Parquet carregado com sucesso do bucket 'bronze'!\n",
      "+---+---+----+------+------+----------------+-------+------+------+\n",
      "|id |age|male|weight|height|bmi             |case_id|futime|status|\n",
      "+---+---+----+------+------+----------------+-------+------+------+\n",
      "|id |age|male|weight|height|bmi             |case.id|futime|status|\n",
      "|1  |57 |0   |60    |163   |22.6909394821587|10630  |6261  |0     |\n",
      "|2  |67 |0   |70.4  |168   |24.8840277061027|14817  |624   |0     |\n",
      "|3  |53 |1   |105.8 |186   |30.4535370503297|3      |1783  |0     |\n",
      "|4  |56 |1   |109.3 |170   |37.8300998173156|6628   |3143  |0     |\n",
      "+---+---+----+------+------+----------------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- male: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- bmi: string (nullable = true)\n",
      " |-- case_id: string (nullable = true)\n",
      " |-- futime: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregar o arquivo Parquet do bucket 'bronze'\n",
    "try:\n",
    "    parquet_df = spark.read.parquet(\"s3a://bronze/nafld1.parquet\")\n",
    "    print(\"Arquivo Parquet carregado com sucesso do bucket 'bronze'!\")\n",
    "    # Exibir as primeiras linhas do DataFrame Parquet\n",
    "    parquet_df.show(5, truncate=False)\n",
    "    # Exibir o esquema para confirmar os tipos de dados\n",
    "    parquet_df.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o arquivo Parquet do bucket 'bronze': {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
